{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16, ResNet50, MobileNetV2\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Flatten, Softmax, ReLU, Dropout, Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.backend import clear_session \n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from Functions import process_img, get_label\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# グループプロジェクトで行なったことのまとめ(モデル作成）\n",
    "# 目次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **学習データ集める**  \n",
    "    1.画像を得る方法  \n",
    "    2.ラズベリーパイの操作  \n",
    "  \n",
    "4. **モデル作成**  \n",
    "   1.転移学習\n",
    "       1.使用したモデル\n",
    "       2.精度が上がらないとき\n",
    "       3.モデルもメモリが大きすぎて困った時\n",
    "       \n",
    "   2.異常検知\n",
    "       1.採用した方法\n",
    "       2.閾値の設定\n",
    "       3.インスタンスの保存\n",
    "       4.本番ではかなりずれた\n",
    "       \n",
    "5. **反省**\n",
    "6. **番外編**  \n",
    "    1.ラズベリーパイにsklearnをインストールした  \n",
    "    2.Unet  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習データを集める\n",
    "\n",
    "## 画像データを得る方法  \n",
    "モデルに学習させるデータは、実際にラズベリーパイが取得するデータの方が望ましい。\n",
    "なぜなら、学習させる画像の大きさや背景などが異なれば精度が落ちることが想定されるためである。\n",
    "そのため、以下の手順でデータを取得した。\n",
    "\n",
    "1. ペットボトルを置く箱を作成する\n",
    "2. カメラを固定する台を作成する\n",
    "3. ラズベリーパイで画像を取得する\n",
    "4. 取得した画像をUSB経由でPCへ保存する\n",
    "\n",
    "※）なお、YoLoV3などの物体検出を用いてペットボトルだけを認識させればこのようなことは必要なかったかもしれない。  \n",
    "しかし、今回は時間やラズベリーパイのメモリを考慮し簡単な画像認識を採用した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラズベリーパイの操作  \n",
    "ラズベリーパイで写真を撮る方法は以下に示す。\n",
    "1. ラズベリーパイのターミナルを起動する\n",
    "2. 以下のコードを入力する  \n",
    "$ raspistill -o test.png -h 400 -w 300  \n",
    "実際のシステムで画像を取得する際は縦400横300で取得するため、引数で指定した。  \n",
    "\n",
    "上記の手順で、学習に使うペットボトル画像及び誤判定に用いるダミー画像を取得した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル作成\n",
    "## 転移学習\n",
    "### 使用したモデル\n",
    "以下のモデルを用いて転移学習をさせた。\n",
    "1. VGG16（馴染みがあったので使用した）\n",
    "2. ResNet50（層が深いため、より精度が上がることを期待した）\n",
    "3. MobileNetV2（前期生のプロジェクトで使用されていたため）\n",
    "\n",
    "パラメータを変えながら複数回試したが、一番良かったものを以下に示す  \n",
    "結果的にはMobileNetV2が採用になった。\n",
    "理由としては、精度が高くモデルの容量が小さいことが挙げられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weights</th>\n",
       "      <th>trainable</th>\n",
       "      <th>FC layers</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VGG16</th>\n",
       "      <td>imagenet</td>\n",
       "      <td>all layers</td>\n",
       "      <td>256 128 5</td>\n",
       "      <td>1e-7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ResNet50</th>\n",
       "      <td>imagenet</td>\n",
       "      <td>only FC</td>\n",
       "      <td>256 128 5</td>\n",
       "      <td>2.7e-7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MobileNetV2</th>\n",
       "      <td>imagenet</td>\n",
       "      <td>only FC</td>\n",
       "      <td>256 128 5</td>\n",
       "      <td>2.6e-5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              weights   trainable  FC layers val_loss  val_acc\n",
       "VGG16        imagenet  all layers  256 128 5     1e-7      1.0\n",
       "ResNet50     imagenet     only FC  256 128 5   2.7e-7      1.0\n",
       "MobileNetV2  imagenet     only FC  256 128 5   2.6e-5      1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "models = [\"VGG16\", \"ResNet50\", \"MobileNetV2\"]\n",
    "result_dict = {\"weights\" : [\"imagenet\", \"imagenet\", \"imagenet\"], \"trainable\" : [\"all layers\", \"only FC\", \"only FC\"],\n",
    "               \"FC layers\": [\"256 128 5\", \"256 128 5\", \"256 128 5\"], \"val_loss\" :[\"1e-7\", \"2.7e-7\",\"2.6e-5\"], \"val_acc\" :[1.0, 1.0, 1.0]}\n",
    "result_df = pd.DataFrame(result_dict, index = models)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルを作成するときに使用したコードの例\n",
    "\n",
    "clear_session()\n",
    "\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "mobile = MobileNetV2(include_top=False, weights=\"imagenet\", input_tensor=input_tensor)\n",
    "\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=mobile.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation=\"relu\"))\n",
    "top_model.add(Dense(128, activation=\"relu\"))\n",
    "top_model.add(Dense(5))\n",
    "top_model.add(Softmax())\n",
    "\n",
    "mobile_model = Model(inputs=mobile.input, outputs=top_model(mobile.output))\n",
    "\n",
    "mobile_model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=Adam(lr=1e-4),\n",
    "          metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 精度が上がらないとき\n",
    "\n",
    "精度が上がらないときは以下を試してみた  \n",
    "\n",
    "1. **学習率を変える**  \n",
    "学習率を1e-3 ~ 1e-7まで試して精度を確認した\n",
    "\n",
    "2. **学習する層を変える**  \n",
    "転移学習させてきた重みの学習をする層を増やした。（コードは下記に記載する）\n",
    "\n",
    "3. **FC層を増やしてみる**  \n",
    "FC層を増やして学習をしてみた  \n",
    "\n",
    "4. **モデルを変える**  \n",
    "学習させるモデルを変更した  \n",
    "\n",
    "5. **学習させるデータを増やすもしくは変更する**  \n",
    "ラズベリーパイで新たに画像を取り直したりした。\n",
    "\n",
    "以上を試してみたが、学習率を変えるのが最も簡単かつ変化が大きかった  \n",
    "ただ、実際には学習させるデータを増やしたりすることを中心に行なった  \n",
    "その方が、モデルの頑強性が向上すると考えたためである"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングするコード\n",
    "# 以下のコードをモデルを宣言してからコンパイルする間に挿入する\n",
    "mobile.trainalbe = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルのメモリが大きすぎて困ったとき\n",
    "\n",
    "モデルは下記の「モデル保存①」のコードで重みとともに保存することができるが、そのまま保存すると200MBを超えてしまう。  \n",
    "モデルの容量が大きすぎるとラズベリーパイで開くことができないという問題に直面した。  \n",
    "そこで、「モデル保存②」のコードで保存し直すことで、約75MBまで落とすことができた。  \n",
    "ただ、注意が必要なのはこの保存方法は再学習することができなくなり、推定専用となる。  \n",
    "保存したモデルは「モデル読み込み」コードで読み込むことができる。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morishuuya/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "#モデル保存①\n",
    "mobile_model.save(\"mobile.h5\")\n",
    "\n",
    "#モデル保存②\n",
    "mobile_model.save(\"mobile.h5\", include_optimizer=False)\n",
    "\n",
    "#モデル読み込み\n",
    "model = load_model(\"mobile.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 異常検知 \n",
    "### 採用した方法\n",
    "登録されていない商品は「登録されていない」と表示させるシステムを作成するにあたって、以下の方法が提案された。  \n",
    "1. Softmaxの値を閾値で区切る\n",
    "2. PCAのエンコーダ、デコーダ機能を使って差分をみる\n",
    "3. オートエンコーダを作成し差分を見る。\n",
    "\n",
    "1提案はSoftmaxの値が全て、０.8以下だった場合「登録されていない」と返す方法である。  \n",
    "この方法はSoftmaxで出力される値が「全て自信ない」状態であれば登録されていないだろうという仮説の基成り立っている。  \n",
    "しかしこれは、ダミー画像を推定した結果がとある商品で高確率を返してきたことから却下された。\n",
    "\n",
    "2の提案は入力画像をPCAで次元圧縮を行い、そこから次元をもとに戻したものを出力する。その入力と出力の差分を測定する方法である。  \n",
    "まず、学習データでPCAを学習させる。そのあと、学習データでの次元圧縮、次元復元を行い差分を見る。  \n",
    "ダミーデータはPCAの学習は行わず、次元圧縮と次元復元の差分だけをみる。  \n",
    "学習データとダミーデータとの間を閾値として異常検知を行う。  \n",
    "ダミーデータではうまく復元ができず、差分が大きくなることを利用している。\n",
    "結果的にこの方法が採用となった。\n",
    "\n",
    "3の提案は原理としては2と同じだが、PCAを使わずオートエンコーダのモデルを作成する方法である。  \n",
    "Unetを用いた方法を試行したが、ダミーデータとの差分が出ず却下となった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 閾値の設定\n",
    "学習データの最大値とダミーデータの最小値の間を閾値として設定した。\n",
    "以下はその一部を示す。\n",
    "\n",
    "また、PCAのn_componentsは大きければ大きいほど学習データとダミーデータとの差は大きくなるが、同時にメモリも大きくなる。  \n",
    "n_componentsはいろはすは40、それ以外は30で設定した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=30, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データのロード、準備\n",
    "irohasu_list = glob.glob(\"./../dataset/version3/3_irohasu/\" + \"/*\" + \".png\")\n",
    "irohasu_img = process_img(irohasu_list)\n",
    "irohasu_img_re = irohasu_img.reshape(60, -1)\n",
    "\n",
    "#ダミーデータのロード、準備\n",
    "dummy_list = glob.glob(\"./../dataset/version3/dummy/dummy_i_\" + \"*\" + \".png\")\n",
    "dummy_img_i = process_img(dummy_list)\n",
    "\n",
    "\n",
    "# pcaインスタンスの作成、学習\n",
    "pca_irohasu30 = PCA(n_components = 30)\n",
    "pca_irohasu30.fit(irohasu_img_re)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データの差分の最大値: 785.0\n",
      "ダミーデータの差分の最小値: 6747.0\n"
     ]
    }
   ],
   "source": [
    "#閾値を測定するコード\n",
    "#学習データの最大値、ダミーデータの最小値の間が閾値となる\n",
    "\n",
    "#学習データ最大値の測定\n",
    "s=[]\n",
    "diff = 0\n",
    "dataset = irohasu_img\n",
    "pca_m = pca_irohasu30\n",
    "for data in dataset:\n",
    "    reshape = data.reshape(1, -1)\n",
    "    pca_data = pca_m.transform(reshape)\n",
    "    pca_redata = pca_m.inverse_transform(pca_data)\n",
    "    diff = np.sum(np.abs(reshape - pca_redata))\n",
    "    s.append(diff)\n",
    "\n",
    "print(\"学習データの差分の最大値:\", max(s).round())\n",
    "\n",
    "#ダミーデータの最小値の測定\n",
    "d_list = []\n",
    "for img in dummy_img_i:\n",
    "    d_res = img.reshape(1, -1)\n",
    "    d_pca = pca_m.transform(d_res)\n",
    "    d_pca = pca_m.inverse_transform(d_pca)\n",
    "    d_diff = np.sum(np.abs(d_res - d_pca))\n",
    "    d_list.append(d_diff)\n",
    "print(\"ダミーデータの差分の最小値:\", min(d_list).round())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCAインスタンスの保存\n",
    "学習したPCAを保存し、ラズベリーパイに移行させる必要があったため、pklファイルに保存した。  \n",
    "そのコードを以下に示す。  \n",
    "同様に読み込みのコードも示す  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pca_irohasu.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pca_irohasu30, f) #保存\n",
    "    \n",
    "    \n",
    "with open(\"pca_irohasu.pkl\", \"rb\") as f:\n",
    "            pca_irohasu30 = pickle.load(f)#読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本番ではかなりずれた\n",
    "\n",
    "取得した画像データをPCで処理する際にはそれほど精度は悪くなかった。  \n",
    "何度か試行したが、20回に一度誤認する程度であった。  \n",
    "しかし、ラズベリーパイのシステム中で取得した画像データを処理するとPCAの差分がこれまでと一桁変わってしまった。  \n",
    "なので、実際の精度がかなり落ちてしまった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反省点\n",
    "\n",
    "* ラズベリーパイで取得する画像データがどのようなものなのかを確認してから、モデル作成をするべきだった。  \n",
    "ラズベリーパイの取得画像の大きさ確認やカメラ角度を固定する前にモデル作成してしまったため、無駄な仕事が増えてしまった。\n",
    "* ラズベリーパイが開くことのできる容量を確認すべきだった。  \n",
    "モデルの容量が大きすぎてラズベリーパイが開くことができず、この解決にまた時間を要してしまった。\n",
    "* pcaの閾値をラズベリーパイの取得画像で決めるべきだった  \n",
    "pcaの閾値をpcで作ってしまったため、かなりずれてしまった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 番外編\n",
    "## ラズベリーパイにsklearnをインストールした\n",
    "\n",
    "ラズベリーパイにsklearnがインストールされてなく、pcaが使用できなかったため以下の手順でskleanをインストールした。\n",
    "1. 元となる仮想環境でパッケージをtxtファイルに保存\n",
    "2. 新しい仮想環境を作る\n",
    "3. 新しい仮想環境でパッケージをインストール\n",
    "4. tensorflow, keras, scikit-learnを個別でインストール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unet\n",
    "unetを用いたオートエンコーダにクラス分類を付随したモデルを作成してみた。  \n",
    "が、精度が悪く、モデルが大きすぎるので却下した。    \n",
    "供養の意味で以下にコードをのせる   \n",
    "層やフィルタの数が足りない可能性が高いが、試してはいない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet():\n",
    "    clear_session()\n",
    "    #インプット\n",
    "    main_input = Input((224, 224, 3))\n",
    "    \n",
    "    #エンコード\n",
    "    conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=\"conv1_1\")(main_input)\n",
    "    conv1 = Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=\"conv1_2\")(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2), name=\"pool1\")(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=\"conv2_1\")(pool1)\n",
    "    conv2 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=\"conv2_2\")(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2), name=\"pool2\")(conv2)\n",
    "    \n",
    "    #最下層\n",
    "    conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=\"conv3_1\")(pool2)\n",
    "    conv3 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', name=\"conv3_2\")(conv3)\n",
    "    drop3 = Dropout(0.5,name=\"drop3\")(conv3)\n",
    "    \n",
    "    #デコード\n",
    "    up4 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal',)(UpSampling2D(size = (2,2))(drop3))\n",
    "    merge4 = concatenate([conv2,up4], axis = 3)\n",
    "    conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge4)\n",
    "    conv4 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "    \n",
    "    up5 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv4))\n",
    "    merge5 = concatenate([conv1,up5], axis = 3)\n",
    "    conv5 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge5)\n",
    "    conv5 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "    \n",
    "    #デコードの出力\n",
    "    main_out = Conv2D(3, 1, padding = 'same', kernel_initializer = 'he_normal', name = \"main_out\")(conv5)\n",
    "    \n",
    "    \n",
    "    #クラス分類\n",
    "    class1 = Flatten()(drop3)\n",
    "    class2 = Dense(128, activation=\"relu\")(class1)\n",
    "    class_out = Dense(5, activation=\"softmax\", name = \"class_out\")(class2)\n",
    "    \n",
    "    #モデルの作成\n",
    "    model =  Model(input = main_input , output = [main_out, class_out])\n",
    "    \n",
    "    #モデルのコンパイル\n",
    "    model.compile(optimizer = Adam(lr = 1e-4), loss = {\"main_out\" :\"mse\", \"class_out\" :'categorical_crossentropy'},\n",
    "                  metrics = {\"main_out\" :\"mse\", \"class_out\" : \"accuracy\"})\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
